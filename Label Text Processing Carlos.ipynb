{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"label_data/label_text_Adrian.csv\"\n",
    "df = pd.read_csv(file_path, header = None)\n",
    "# Dropping repeat label\n",
    "df = df.drop(10).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load pretrained models\n",
    "weight_pipe= pipeline(\"ner\", model=\"d4data/biomedical-ner-all\", aggregation_strategy=\"average\")\n",
    "food_pipe = pipeline(\"ner\", model=\"Dizex/InstaFoodRoBERTa-NER\", aggregation_strategy=\"max\")\n",
    "date_pipe = pipeline(\"ner\", model=\"mdarhri00/named-entity-recognition\", aggregation_strategy=\"average\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_extraction(text):\n",
    "\n",
    "    # Apply the pretrained models\n",
    "    lab_value_ents = weight_pipe(text, aggregation_strategy=\"average\")\n",
    "    date_ents = date_pipe(text)\n",
    "    food_ents = food_pipe(text)\n",
    "\n",
    "\n",
    "    food_name = np.nan\n",
    "    packaged_date = np.nan\n",
    "    expiry_date = np.nan\n",
    "    weight = np.nan\n",
    "\n",
    "\n",
    "    # Getting Dates\n",
    "    #print(\"DATE ENT LIST:\")\n",
    "    for ent in date_ents:\n",
    "        if ent['entity_group'] == \"date_time\":\n",
    "            #print(f\"ent: {ent['word']}, class: {ent['entity_group']}\")\n",
    "            start = ent[\"start\"]\n",
    "            end = ent[\"end\"]\n",
    "            \n",
    "            # Get the tokens before and after the DATE entity\n",
    "            before_ent = text[start-20:start]\n",
    "            after_ent = text[end:end+5]\n",
    "            \n",
    "            # Check the context around the DATE entity\n",
    "            context = before_ent.lower() + \" \" + after_ent.lower()\n",
    "\n",
    "            # Define the phrases to look for\n",
    "            packaging_phrases = [\"packed\", \"pkg on\", \"packaged on\", 'production date']\n",
    "            expiry_phrases = [\"best before\", \"use by\", \"expiry date\", \"expires on\", \"expires by\", 'expires', 'after']\n",
    "\n",
    "            if any(phrase in context for phrase in expiry_phrases):\n",
    "                expiry_date = ent[\"word\"]\n",
    "            elif any(phrase in context for phrase in packaging_phrases):\n",
    "                packaged_date = ent[\"word\"]\n",
    "\n",
    "\n",
    "    # Getting Weight\n",
    "\n",
    "    # print(\"\")\n",
    "    #print(\"WEIGHT ENT LIST:\")\n",
    "    #[print(f\"ent: {ent['word']}, score: {ent['score']}, class: {ent['entity_group']}\") for ent in lab_value_ents]\n",
    "    weight_phrases = ['0g', '1g','2g','3g','4g','5g','6g','7g','8g', '9g', 'kg','grams','kilograms']\n",
    "    banned_weight_phrases = [\"-\", \"kg \", \"g \"]\n",
    "\n",
    "    weight_entities = [ent for ent in lab_value_ents if any(phrase in ent['word'].lower() for phrase in weight_phrases) and not any(phrase in ent['word'].lower() for phrase in banned_weight_phrases)]\n",
    "    #[print(f\"ent: {ent['word']}, score: {ent['score']}\") for ent in weight_entities]\n",
    "    if weight_entities == []:\n",
    "        pass\n",
    "    else:\n",
    "        max_score_entity = max(weight_entities, key=lambda entity: entity['score'])\n",
    "        weight = max_score_entity[\"word\"]\n",
    "\n",
    "\n",
    "    # Getting Food Name\n",
    "\n",
    "    # print(\"\")\n",
    "    # print(\"FOOD ENT LIST:\")\n",
    "    #[print(f\"ent: {ent['word']}, score: {ent['score']}\") for ent in food_ents]\n",
    "    if food_ents == []:\n",
    "        pass\n",
    "    else:\n",
    "        max_score_entity = max(food_ents, key=lambda entity: entity['score'])\n",
    "        food_name = max_score_entity[\"word\"]\n",
    "\n",
    "\n",
    "    return expiry_date, packaged_date, weight, food_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Food Name:  Australia Beef Burger Bulk Pack\n",
      "Weight: 13. 63 kg\n",
      "Packaged Date: 14 May 2024\n",
      "Expiry Date: 14 May 2025\n",
      "\n",
      "Example Text: Keep Frozen Product of Australia Beef Burger Bulk Pack Net Wt 13.63 kg 13.63 KG Pkg On 14 May 2024 Best Before 14 May 2025 Top Cut Foods Pty Ltd 101265 Boneless Beef NL Allergens\n"
     ]
    }
   ],
   "source": [
    "# Testing out the model on one example\n",
    "\n",
    "text = df[0][1]\n",
    "\n",
    "expiry_date, packaged_date, weight, food_name = text_extraction(text)\n",
    "\n",
    "print(\"\")\n",
    "print(f\"Food Name: {food_name}\")\n",
    "print(f\"Weight: {weight}\")\n",
    "print(f\"Packaged Date: {packaged_date}\")\n",
    "print(f\"Expiry Date: {expiry_date}\")\n",
    "print(\"\")\n",
    "print(f\"Example Text: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "# Loop through each text in the dataframe\n",
    "for text in df[0]:\n",
    "    expiry_date, packaged_date, weight, food_name = text_extraction(text)\n",
    "    data.append([food_name, weight, expiry_date])\n",
    "\n",
    "# Create a new dataframe with the extracted data\n",
    "extracted_df = pd.DataFrame(data, columns=[\"Product Name\", \"Weight\", \"Expiry Date\"])\n",
    "\n",
    "# Write the dataframe to a CSV file\n",
    "extracted_df.to_csv('label_data/extracted_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the predicted Product Names, Weights, and Expiry Dates\n",
    "file_path = \"label_data/extracted_data.csv\"\n",
    "predicted_df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the correctly annotated data which the predictions will be measure against for accuracy\n",
    "file_path = \"label_data/annotated_adrian.csv\"\n",
    "annotated_df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil import parser\n",
    "from datetime import datetime\n",
    "\n",
    "def convert_date(date_list):\n",
    "\n",
    "    formatted_list = []\n",
    "\n",
    "    # Parse the date using dateutil.parser\n",
    "    for date_str in date_list:\n",
    "        try:\n",
    "            parsed_date = parser.parse(date_str)\n",
    "            # Format the date to the desired format: e.g '23 Jul 2003'\n",
    "            formatted_date = parsed_date.strftime('%d %b %Y')\n",
    "            formatted_list.append(formatted_date)\n",
    "        except TypeError:\n",
    "            formatted_list.append(date_str)\n",
    "\n",
    "        \n",
    "    \n",
    "    return formatted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_df['Expiry Date'] = convert_date(predicted_df['Expiry Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import hamming\n",
    "\n",
    "# Cosine Similarity Accuracy Measure\n",
    "\n",
    "def text_similarity(text1, text2):\n",
    "    \"\"\"Calculate the cosine similarity between two texts using TF-IDF.\"\"\"\n",
    "    if pd.isna(text1) and pd.isna(text2):\n",
    "        return 100  # Both are NaN, consider it 100% accurate\n",
    "    elif pd.isna(text1) or pd.isna(text2):\n",
    "        return 0  # One is NaN, consider it 0% accurate\n",
    "    else:\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
    "        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "        return similarity[0][0] * 100\n",
    "    \n",
    "#Levenshtein Distance Accuracy Measure\n",
    "\n",
    "def levenshtein_distance(s1, s2):\n",
    "    \"\"\"Calculate the Levenshtein Distance between two strings.\"\"\"\n",
    "    if len(s1) > len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "\n",
    "    distances = range(len(s1) + 1)\n",
    "    for index2, char2 in enumerate(s2):\n",
    "        new_distances = [index2 + 1]\n",
    "        for index1, char1 in enumerate(s1):\n",
    "            cost = 0 if char1 == char2 else 1\n",
    "            new_distances.append(min(\n",
    "                distances[index1] + 1,\n",
    "                new_distances[-1] + 1,\n",
    "                distances[index1] + cost\n",
    "            ))\n",
    "        distances = new_distances\n",
    "    return distances[-1]\n",
    "\n",
    "def similarity_percentage(s1, s2):\n",
    "    \"\"\"Calculate the similarity percentage between two strings.\"\"\"\n",
    "    if s1 == s2:\n",
    "        return 100.0\n",
    "    elif pd.isna(s1) or pd.isna(s2):\n",
    "        return 0  # One is NaN, consider it 0% accurate\n",
    "\n",
    "    lev_distance = levenshtein_distance(s1, s2)\n",
    "    max_len = max(len(s1), len(s2))\n",
    "    similarity = (1 - lev_distance / max_len) * 100\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_pred_acc(predicted_df, annotated_df, acc_measure = 'cosine'):\n",
    "    \n",
    "    food_acc = 0\n",
    "    weight_acc = 0\n",
    "    date_acc = 0\n",
    "    nrows = predicted_df.shape[0]\n",
    "\n",
    "    if acc_measure == 'cosine':\n",
    "        for i in range(nrows):\n",
    "                food_acc += text_similarity(predicted_df.iloc[i]['Product Name'], annotated_df.iloc[i]['Product Name'])\n",
    "                weight_acc += text_similarity(predicted_df.iloc[i]['Weight'], annotated_df.iloc[i]['Weight'])\n",
    "                date_acc += text_similarity(predicted_df.iloc[i]['Expiry Date'], annotated_df.iloc[i]['Expiry Date'])\n",
    "    elif acc_measure == 'lev': \n",
    "         for i in range(nrows):\n",
    "                food_acc += similarity_percentage(predicted_df.iloc[i]['Product Name'], annotated_df.iloc[i]['Product Name'])\n",
    "                weight_acc += similarity_percentage(predicted_df.iloc[i]['Weight'], annotated_df.iloc[i]['Weight'])\n",
    "                date_acc += similarity_percentage(predicted_df.iloc[i]['Expiry Date'], annotated_df.iloc[i]['Expiry Date'])\n",
    "    else:\n",
    "          raise ValueError(\"Please enter a valid Accuracy Measure\")\n",
    "\n",
    "    food_acc = food_acc/nrows\n",
    "    weight_acc = weight_acc/nrows\n",
    "    date_acc = date_acc/nrows\n",
    "\n",
    "    return food_acc, weight_acc, date_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Food Name Accuracy = 48.798345964974686\n",
      "Weight Accuracy = 48.340334879649774\n",
      "Date Accuracy = 62.404221307443024\n",
      "Overall Accuracy = 53.180967384022495\n"
     ]
    }
   ],
   "source": [
    "food_acc, weight_acc, date_acc = label_pred_acc(predicted_df, annotated_df)\n",
    "\n",
    "print(f\"Food Name Accuracy = {food_acc}\")\n",
    "print(f\"Weight Accuracy = {weight_acc}\")\n",
    "print(f\"Date Accuracy = {date_acc}\")\n",
    "print(f\"Overall Accuracy = {(food_acc + weight_acc + date_acc)/3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
